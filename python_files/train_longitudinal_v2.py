"""V2 staged training entrypoint (triplet-aware, same backbone init as current).

This script is intentionally separate from `longitudinal_MIM_training.py`.
It reuses the SAME backbone encoder module and initializes its weights the
same way as the current training script:

- If `LONGITUDINAL_LOAD_PATH` points to a checkpoint (.pt with `model_dict`),
  it loads the whole model state dict (non-strict by default).
- Else it loads encoder-only weights into `model.backbone.encoder`.

Data:
- Expects triplet directories generated by `CT_entities/DRR_generator_triplet.py`.
  Each sample folder must contain: prior.nii.gz, intermediate.nii.gz, current.nii.gz

Stages:
- A: supervise full diff (current - prior) using diff_map if present
- B: train deformation head to warp prior -> intermediate
- C: supervise entity-only diff (current - intermediate)

"""

from __future__ import annotations

import argparse

import torch
from torch.utils.data import DataLoader

from constants import BATCH_SIZE, LONGITUDINAL_LOAD_PATH, DEVICE

from refactored_modules.longitudinal_v2 import (
    LongitudinalMIMDeformV2,
    StageConfig,
    TrainerV2,
    TripletDRRDataset,
    DRRImageDataset,
)


def load_backbone_like_current(model: LongitudinalMIMDeformV2, *, load_path: str) -> None:
    """Match the weight-init/loading logic from `longitudinal_MIM_training.py`."""
    p = load_path
    if not p:
        print("[v2] LONGITUDINAL_LOAD_PATH is empty; training from scratch.")
        return

    print(f"[v2] Loading backbone init from: {p}")
    ckpt = torch.load(p, map_location="cpu")

    # Heuristic: checkpoint dicts typically have 'model_dict'
    if isinstance(ckpt, dict) and "model_dict" in ckpt:
        strict = False
        print(f"[v2] Detected checkpoint. Loading full model (strict={strict})")
        model.load_state_dict(ckpt["model_dict"], strict=strict)
        return

    # Otherwise treat as encoder-only weights
    print("[v2] Treating load as encoder-only weights")
    model.backbone.encoder.load_state_dict(ckpt, strict=True)


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Train longitudinal v2 (triplet-aware, staged).")
    p.add_argument(
        "--triplet_roots",
        nargs="+",
        required=True,
        help="One or more roots containing triplet sample folders (each has prior/intermediate/current.nii.gz).",
    )
    p.add_argument(
        "--image_roots",
        nargs="+",
        default=None,
        help="Roots for image-only DRRs for encoder pretraining (var*.nii.gz and/or prior/current/intermediate.nii.gz).",
    )
    p.add_argument(
        "--load_path",
        type=str,
        default=LONGITUDINAL_LOAD_PATH,
        help="Initial weights path (checkpoint with model_dict or encoder-only state_dict).",
    )
    p.add_argument("--epochs_A", type=int, default=1)
    p.add_argument("--epochs_B", type=int, default=1)
    p.add_argument("--epochs_C", type=int, default=1)
    p.add_argument("--epochs_E", type=int, default=0, help="If >0, run encoder reconstruction pretraining stage E.")
    p.add_argument("--lr_A", type=float, default=6e-4)
    p.add_argument("--lr_B", type=float, default=3e-4)
    p.add_argument("--lr_C", type=float, default=6e-4)
    p.add_argument("--lr_E", type=float, default=6e-4)
    p.add_argument("--wd", type=float, default=1e-2)
    p.add_argument("--lambda_smooth", type=float, default=0.05)
    p.add_argument(
        "--lambda_recon",
        type=float,
        default=0.0,
        help="Aux reconstruction loss weight (applied in stages A and C).",
    )
    return p.parse_args()


def main() -> None:
    args = parse_args()

    dataset = TripletDRRDataset(args.triplet_roots)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)

    recon_dataloader = None
    if args.epochs_E and args.epochs_E > 0:
        if not args.image_roots:
            raise RuntimeError("Stage E requested (epochs_E>0) but --image_roots not provided.")
        recon_dataset = DRRImageDataset(args.image_roots)
        recon_dataloader = DataLoader(recon_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)

    model = LongitudinalMIMDeformV2(
        use_mask_token=False,
        dec=6,
        patch_dec=False,
        use_pos_embed=False,
        use_technical_bottleneck=False,
        enable_deformation=True,
        enable_reconstruction=True,
    )

    load_backbone_like_current(model, load_path=args.load_path)

    trainer = TrainerV2(model, device=DEVICE)

    # -----------------
    # Stage schedule
    # -----------------
    stages = []
    if args.epochs_E and args.epochs_E > 0:
        stages.append(StageConfig(stage="E", epochs=args.epochs_E, lr=args.lr_E, weight_decay=args.wd))
    stages.extend(
        [
            StageConfig(stage="A", epochs=args.epochs_A, lr=args.lr_A, weight_decay=args.wd, lambda_recon=args.lambda_recon),
            StageConfig(stage="B", epochs=args.epochs_B, lr=args.lr_B, weight_decay=0.0, lambda_smooth=args.lambda_smooth),
            StageConfig(stage="C", epochs=args.epochs_C, lr=args.lr_C, weight_decay=args.wd, lambda_recon=args.lambda_recon),
        ]
    )

    for cfg in stages:
        if cfg.stage == "E":
            assert recon_dataloader is not None
            trainer.train_stage(cfg, recon_dataloader)
        else:
            trainer.train_stage(cfg, dataloader)


if __name__ == "__main__":
    main()
